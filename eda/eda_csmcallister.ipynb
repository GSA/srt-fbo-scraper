{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import SGDClassifier, Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#in order to use SMOTE, you've got to import Pipeline from imblearn\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import dill as pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Labeled Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_df(labeled_data_path):\n",
    "    '''\n",
    "    Create a pandas DataFrame with the labled attachment texts.\n",
    "    \n",
    "    Arguments:\n",
    "        labeled_data_path (str): the directory for the labeled attachment text files.\n",
    "        \n",
    "    Returns:\n",
    "        labeled_df (pandas DataFrame): a dataframe with a column for the file name, \n",
    "                                       the text, and the label (green, yellow or red).\n",
    "    '''\n",
    "    \n",
    "    texts = []\n",
    "    files = []\n",
    "    labels = []\n",
    "    for file in os.listdir(labeled_data_path):\n",
    "        if file.startswith('.'):\n",
    "            continue\n",
    "        else:\n",
    "            files.append(file)\n",
    "            label = file.split('_')[0]\n",
    "            labels.append(label)\n",
    "            file_path = os.path.join(labeled_data_path,file)\n",
    "            #foce utf-8, ignoring erros\n",
    "            with open(file_path, 'r', errors='ignore') as f:\n",
    "                text = f.read()\n",
    "                texts.append(text)\n",
    "    labeled_df = pd.DataFrame(data=[files,texts,labels]).transpose()\n",
    "    labeled_df.columns = ['file','text','label']\n",
    "    return labeled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'labeled_fbo_docs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-91345b0537df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabeled_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_labeled_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labeled_fbo_docs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-4b3871ef3852>\u001b[0m in \u001b[0;36mcreate_labeled_df\u001b[0;34m(labeled_data_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'labeled_fbo_docs'"
     ]
    }
   ],
   "source": [
    "labeled_df = create_labeled_df('labeled_fbo_docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recode labels to numeric, making the minority Green class the positive class\n",
    "labeled_df['target'] = labeled_df['label'].map({'GREEN':1,'YELLOW':0,'RED':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the number of chars per text\n",
    "labeled_df['text'].apply(lambda x: len(x)).describe().apply(lambda x: '%.f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some docs have a whole bunch of text, most of which is probably useless. Let's count the number of tokens per doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the number of unique tokens per doc\n",
    "labeled_df['text'].apply(lambda x: len(set(x.split()))).describe().apply(lambda x: '%.f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One doc has 49,024 unqiue words. That's insane and will make our feature matrix super sparse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "no_nonsense_re = re.compile(r'^[a-zA-Z^508]+$')\n",
    "def strip_nonsense(doc):\n",
    "    \"\"\"\n",
    "    Returns stemmed lowercased alpha-only substrings from a string that are b/w 3 and 17 chars long. \n",
    "    It keeps the substring `508`.\n",
    "    \n",
    "    Parameters:\n",
    "        doc (str): the text of a single FBO document.\n",
    "        \n",
    "    Returns:\n",
    "        words (str): a string of space-delimited lower-case alpha-only words (except for `508`)\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = doc.lower()\n",
    "    doc = doc.split()\n",
    "    words = ''\n",
    "    for word in doc:\n",
    "        m = re.match(no_nonsense_re, word)\n",
    "        if m:\n",
    "            match = m.group()\n",
    "            if match in stop_words:\n",
    "                continue\n",
    "            else:\n",
    "                match_len = len(match)\n",
    "                if match_len <= 17 and match_len >= 3:\n",
    "                    porter = PorterStemmer()\n",
    "                    stemmed = porter.stem(match)\n",
    "                    words += stemmed + ' '\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes awhile, but is totally worth it\n",
    "labeled_df['normalized_text'] = labeled_df['text'].apply(strip_nonsense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df['normalized_text'].apply(lambda x: len(set(x.split()))).describe().apply(lambda x: '%.f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That reduced sparsity by about 70%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "These help visualize classification results and construct the parameter grid in the case of `log_uniform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for plotting classification results\n",
    "def adjusted_classes(y_scores, t):\n",
    "    \"\"\"\n",
    "    This function adjusts class predictions based on the prediction threshold (t).\n",
    "    Will only work for binary classification problems.\n",
    "    \"\"\"\n",
    "    return [1 if y >= t else 0 for y in y_scores]\n",
    "\n",
    "def precision_recall_threshold(p, r, thresholds, t=0.5):\n",
    "    \"\"\"\n",
    "    plots the precision recall curve and shows the current value for each\n",
    "    by identifying the classifier's threshold (t).\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate new class predictions based on the adjusted_classes\n",
    "    # function above and view the resulting confusion matrix.\n",
    "    y_pred_adj = adjusted_classes(y_scores, t)\n",
    "    print(\"=\"*80)\n",
    "    print(\"Operating point = {:.3f}\".format(t),end=\"\\n\\n\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(pd.DataFrame(metrics.confusion_matrix(y_test, y_pred_adj),\n",
    "                       columns=['pred_red', 'pred_gree'], \n",
    "                       index=['red', 'green']),end=\"\\n\\n\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(metrics.classification_report(y_test, y_pred_adj, target_names=['red', 'green']))\n",
    "    \n",
    "    \n",
    "    font = {'family' : 'normal',\n",
    "            'weight' : 'medium',\n",
    "            'size'   : 18}\n",
    "\n",
    "    plt.rc('font', **font)\n",
    "    # plot the curve\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title(\"Precision and Recall curve ^ = current threshold\",fontdict=font)\n",
    "    plt.step(r, p, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(r, p, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "    plt.ylim([0.0, 1.01]);\n",
    "    plt.xlim([0.0, 1.01]);\n",
    "    plt.xlabel('Recall',fontdict=font);\n",
    "    plt.ylabel('Precision',fontdict=font);\n",
    "    \n",
    "    # plot the current threshold on the line\n",
    "    close_default_clf = np.argmin(np.abs(thresholds - t))\n",
    "    plt.plot(r[close_default_clf], p[close_default_clf], '^', c='k',\n",
    "            markersize=15)\n",
    "    \n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    \"\"\"\n",
    "    Modified from:\n",
    "    Hands-On Machine learning with Scikit-Learn\n",
    "    and TensorFlow; p.89\n",
    "    \"\"\"\n",
    "    font = {'family' : 'normal',\n",
    "            'weight' : 'medium',\n",
    "            'size'   : 18}\n",
    "\n",
    "    plt.rc('font', **font)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Precision and Recall Scores as a function of the decision threshold\",fontdict=font)\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.ylabel(\"Score\",fontdict=font)\n",
    "    plt.xlabel(\"Decision Threshold\",fontdict=font)\n",
    "    plt.legend(loc='best',fontsize=16)\n",
    "    \n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    \"\"\"\n",
    "    The ROC curve, modified from \n",
    "    Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title('ROC Curve')\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([-0.005, 1, 0, 1.005])\n",
    "    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "def plot_fpr_recall_vs_threshold(y_test, y_scores):\n",
    "    \"\"\"\n",
    "    Modified from:\n",
    "    Hands-On Machine learning with Scikit-Learn\n",
    "    and TensorFlow; p.89\n",
    "    \"\"\"\n",
    "    def find_nearest(array, value):\n",
    "        array = np.asarray(array)\n",
    "        idx = (np.abs(array - value)).argmin()\n",
    "        return array[idx]\n",
    "    \n",
    "    \n",
    "    fpr, tpr, roc_thresh = metrics.roc_curve(y_test, y_scores)\n",
    "    p, r, prc_thresh = metrics.precision_recall_curve(y_test, y_scores)\n",
    "    \n",
    "    font = {'family' : 'normal',\n",
    "            'weight' : 'medium',\n",
    "            'size'   : 18}\n",
    "\n",
    "    plt.rc('font', **font)\n",
    "   \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"False Positive and Recall Scores as a function of the decision threshold\",fontdict=font)\n",
    "\n",
    "    plt.plot(prc_thresh, r[:-1], \"b--\", label=\"Recall\")\n",
    "    plt.plot(roc_thresh, fpr, \"g-\", label=\"FPR\")\n",
    "    \n",
    "    #plot vertical line where recall is highest (i.e. == 1)\n",
    "    x_text = prc_thresh[0]\n",
    "    plt.axvline(x=x_text)\n",
    "    #plot horizontal line that will intersect where the vertical line hits the fpr line\n",
    "    y_text = fpr[np.where(roc_thresh==find_nearest(roc_thresh, prc_thresh[0]))[0][0]]\n",
    "    plt.axhline(y=y_text)\n",
    "    x = round(x_text,2)\n",
    "    y = round(y_text,2)\n",
    "    plt.text(x_text,y_text,\"({}, {})\".format(x, y))\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Decision Threshold\")\n",
    "    plt.legend(loc='best',fontsize=16)\n",
    "    \n",
    "def plot_prc(y_score, y_test):\n",
    "    \"\"\"\n",
    "    Plot the precision-recall curve, labeled with average precision.\n",
    "    \"\"\"\n",
    "    average_precision = metrics.average_precision_score(y_test, y_score)\n",
    "\n",
    "    precision, recall, _ = metrics.precision_recall_curve(y_test, y_score)\n",
    "    #plot it\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "    plt.xlabel('Recall',fontsize=16)\n",
    "    plt.ylabel('Precision',fontsize=16)\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "              average_precision),fontsize=20)\n",
    "\n",
    "class log_uniform():        \n",
    "    \"\"\"\n",
    "    Provides an instance of the log-uniform distribution with an .rvs() method. Meant to be used with \n",
    "    RandomizedSearchCV, particularly for params like alpha, C, gamma, etc. \n",
    "    \n",
    "    Attributes:\n",
    "        a (int or float): the exponent of the beginning of the range \n",
    "        b (int or float): the exponent of the end of range. \n",
    "        base (int or float): the base of the logarithm. 10 by default.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, a=-1, b=0, base=10):\n",
    "        self.loc = a\n",
    "        self.scale = b - a\n",
    "        self.base = base\n",
    "\n",
    "    def rvs(self, size=1, random_state=None):\n",
    "        uniform = stats.uniform(loc=self.loc, scale=self.scale)\n",
    "        return np.power(self.base, uniform.rvs(size=size, random_state=random_state))    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Grid Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomized_grid_search(df, weight_classes = True, n_iter_search = 500, score='roc_auc',\n",
    "                           pickle_best=True, random_state = 123, pickle_file = None):\n",
    "    \"\"\"\n",
    "    Given labeled training data (`df`) for a binary classification task, performs a \n",
    "    randomized grid search `n_iter_search` times using SGDClassifier() and the `score`\n",
    "    as a scoring metric. Prints clf report and plots roc and prc curves. Optionally pickles best classifier.\n",
    "    \n",
    "    Attributes:\n",
    "        df (pandas DataFrame):  the training data. Currently, you must specify within the function\n",
    "                                label and feature column names.\n",
    "        weight_classes (bool): whether or not to use the “balanced” mode to adjust class weights.\n",
    "        n_iter_search:  number of parameter settings that are sampled. Trades off runtime vs quality \n",
    "                        of the solution.\n",
    "        score (str):  the scorer used to evaluate the predictions on the test set. `roc_auc` by\n",
    "                      default. Available options include:  accuracy, roc_auc, precision, fbeta, recall.\n",
    "                      Note: for fbeta, beta is set to 1.5 to favor recall of the positive class.\n",
    "        pickle_best (bool): whether or not to pickle the best model\n",
    "        random_state (123): sets the random seed for reproducibility\n",
    "        pickle_path (str): what to name the pickled classifier. The scorer chosen gets appended to this name.\n",
    "    \"\"\"\n",
    "    if weight_classes:\n",
    "        clf = SGDClassifier(class_weight = 'balanced')\n",
    "    else:\n",
    "        clf = clf=SGDClassifier()\n",
    "    \n",
    "    scoring = {'accuracy': metrics.make_scorer(metrics.accuracy_score),\n",
    "               'roc_auc': metrics.make_scorer(metrics.roc_auc_score),\n",
    "               'precision': metrics.make_scorer(metrics.average_precision_score),\n",
    "               'fbeta':metrics.make_scorer(metrics.fbeta_score,beta=.5),\n",
    "               'recall':metrics.make_scorer(metrics.recall_score)}\n",
    "    \n",
    "    clf_name = clf.__class__.__name__\n",
    "    X = df['normalized_text']\n",
    "    y = df['target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y,\n",
    "                                                        stratify=y,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=random_state)\n",
    "    \n",
    "    pipe = Pipeline([('vectorizer', TfidfVectorizer(stop_words='english')),\n",
    "                     ('select', SelectKBest(chi2)),\n",
    "                     ('clf', clf)])\n",
    "    \n",
    "    param_dist = {\n",
    "                  \"vectorizer__ngram_range\":[(1,1), (1,2)],\n",
    "                  \"vectorizer__min_df\":stats.randint(1,3),\n",
    "                  \"vectorizer__max_df\":stats.uniform(.95,.3),\n",
    "                  \"vectorizer__sublinear_tf\":[True, False],\n",
    "                  \"select__k\":[10,100,200,500,1000,1500,2000,5000],\n",
    "                  \"clf__alpha\": log_uniform(-5,2),\n",
    "                  \"clf__penalty\": ['l2','l1','elasticnet'],\n",
    "                  \"clf__loss\": ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "                  }\n",
    "    \n",
    "    random_search = RandomizedSearchCV(pipe, \n",
    "                                       param_distributions = param_dist, \n",
    "                                       scoring = scoring, \n",
    "                                       refit = score,\n",
    "                                       n_iter = n_iter_search, \n",
    "                                       cv = 5,\n",
    "                                       n_jobs = -1, \n",
    "                                       verbose = 1, \n",
    "                                       random_state = random_state)\n",
    "    \n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = random_search.predict(X_test)\n",
    "    #get the col number of the positive class (i.e. green)\n",
    "    positive_class_col = list(random_search.classes_).index(1)\n",
    "    try:\n",
    "        y_score = random_search.predict_proba(X_test)[:,positive_class_col]\n",
    "    except AttributeError:\n",
    "        y_score = random_search.decision_function(X_test)\n",
    "    average_precision = metrics.average_precision_score(y_test, y_score)\n",
    "    acc = metrics.accuracy_score(y_test,y_pred)\n",
    "    roc_auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "    precisions, recalls, _ = metrics.precision_recall_curve(y_test, y_score)\n",
    "    auc = metrics.auc(recalls, precisions)\n",
    "    fbeta = metrics.fbeta_score(y_test,y_pred,beta=1.5)\n",
    "    recall = metrics.recall_score(y_test,y_pred)\n",
    "\n",
    "    print(\"\\tRecall on test data:  {0:.2f}\".format(recall))\n",
    "    print(\"\\tAccuracy on test data:  {0:.2f}\".format(acc))\n",
    "    print(\"\\tROC-AUC on test data:  {0:.2f}\".format(roc_auc))\n",
    "    print(\"\\tFbeta on test data:  {0:.2f}\".format(fbeta))\n",
    "    print(\"\\tAverage Precision on test data:  {0:.2f}\".format(average_precision))\n",
    "    print(\"\\tPrecision-Recall AUC on test data:  {0:.2f}\".format(auc))\n",
    "    print(\"-\"*80)\n",
    "    print(\"Classification Report:\")\n",
    "    class_names = ['red', 'green']\n",
    "    print(metrics.classification_report(y_test, y_pred, target_names=class_names))\n",
    "    print(\"-\"*80)\n",
    "    plot_prc(y_score, y_test)\n",
    "    print(\"-\"*80)\n",
    "    fpr, tpr, auc_thresholds = metrics.roc_curve(y_test, y_score)\n",
    "    print(\"AUC of ROC:  {0:.4f}\".format(metrics.auc(fpr, tpr)))\n",
    "    plot_roc_curve(fpr, tpr, 'ROC Curve')\n",
    "    best_estimator = random_search.best_estimator_\n",
    "    best_score = random_search.best_score_\n",
    "    result_values = [y_pred, y_score, precisions, recall, average_precision,\n",
    "                     acc, roc_auc, auc, fbeta, recalls, best_score, best_estimator, y_test]\n",
    "    result_keys = ['y_pred', 'y_score', 'precisions', 'recall', 'average_precision','acc',\n",
    "                   'roc_auc', 'auc', 'fbeta', 'recalls','best_score','best_estimator','y_test']\n",
    "    results = {k:v for k,v in zip(result_keys,result_values)}\n",
    "    \n",
    "    if pickle_best:\n",
    "        pickle_path = os.path.join(os.getcwd(),pickle_file+score+'.pkl')\n",
    "        with open(pickle_path, 'wb') as f: \n",
    "            pickle.dump(random_search.best_estimator_, f) \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = randomized_grid_search(labeled_df,\n",
    "                                 n_iter_search = 300,\n",
    "                                 score = 'accuracy',\n",
    "                                 pickle_best = True,\n",
    "                                 random_state = 123,\n",
    "                                 pickle_file = 'clf_csmcallister_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare against Dummy Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dummy_clf_report(X_train, y_train, X_test, y_test):\n",
    "    for strategy in ['stratified','most_frequent','prior','uniform']:\n",
    "        print(\"=\"*80)\n",
    "        print(strategy)\n",
    "        print(\"=\"*80)\n",
    "        dummy = DummyClassifier(strategy=strategy)\n",
    "        dummy.fit(X_train, y_train)\n",
    "        y_pred = dummy.predict(X_test)\n",
    "        print(metrics.classification_report(y_test, y_pred, target_names=['red', 'green']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros(shape=labeled_df.shape)\n",
    "y = labeled_df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=123)\n",
    "print_dummy_clf_report(X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
