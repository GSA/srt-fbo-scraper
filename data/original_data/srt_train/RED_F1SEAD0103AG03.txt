



ADDENDUM TO 52.212-1 PROPOSAL PREPARATION INSTRUCTIONS



A.  To assure timely and equitable evaluation of the proposal, the offeror must follow the instructions contained herein. The proposal must be complete, self-sufficient, and respond directly to the requirements of thifiresoys solicitation.  The response shall consist of two (3) separate parts; Part I – Price, Part II – Past Performance Information,  Part III – Pass/Fail Technical Evaluation.

Format for proposal both parts shall be as follows:  A page is defined as one face of an 8 ½” x 11” sheet of paper containing information.  Typing shall not be less than 12 pitch.



B.  The contracting officer has determined there is a high probability of adequate price competition in this acquisition. Upon examination of the initial offers, the contracting officer will review this determination and if, in the contracting officer’s opinion, adequate price competition exists no additional cost information will be requested and certification under  will not be required.  However, if at any time during this competition the contracting officer determines that adequate price competition no longer exists offerors may be required to submit information other than cost or pricing data to support a determination of price reasonableness.



C.  Specific Instructions:



1.  PART I – PRICE PROPOSAL - The offeror agrees to hold the prices in its offer firm for 90 calendar days from the date specified for receipt of offers.  Submit original and one (1) copy and one (1) electronic copy in Microsoft Excel.  The number of pages for the price proposal will be (2) pages. 



2.  PART II – PAST PERFORMANCE INFORMATION - Only references for same or similar type contracts are desired.  Submit original and one (1) copy.  Pages limited to no more than two (2) pages total. 



(a)  Quality and Satisfaction Rating for Contracts Completed in the Past Three Years (from the solicitation issue date): Provide information currently available (letters, metrics, customer surveys, independent surveys, etc.) which demonstrates customer satisfaction with overall job performance and quality of completed product for same or similar type contract. In addition, explain corrective actions taken in the past, if any, for substandard performance and any current performance problems such as cost overruns, extended performance periods, numerous warranty calls, etc.

(b)  Past Performance Surveys: The government will evaluate the quality and extent of offeror’s performance deemed relevant to the requirements of this solicitation. The government will use information submitted by the offeror and other sources such as other government offices and commercial sources, to assess performance.  Provide a list of no more than three (3) for the Prime contractor and three (3) for the any subcontractor who will perform significant work on this acquisition of the most relevant contracts performed for government agencies and commercial customers within the last three (3) years from the solicitation issue date. Relevant contracts are described in the addendum to FAR 52.212-2 of this solicitation.  The evaluation of past performance information will take into account past performance information regarding predecessor companies and subcontractors that will perform major or critical aspects of the requirement when such information is relevant to the instant acquisition.  Furnish the following information for each contract referenced:



(i) Company/Division name

(ii) Product/Service

(iii) Contracting Agency/Customer

(iv) Contract Number

(v) Contract Dollar Value

(vi) Period of Performance

(vii) Verified, up-to-date name, address, FAX and telephone number of the contracting officer

(viii) Comments regarding compliance with contract terms and conditions

(ix) Comments regarding any known performance deemed unacceptable to the customer, or not in accordance with the contract terms and conditions.



If a teaming arrangement is contemplated, provide complete information as to the arrangement, including any relevant and recent past/present performance information on previous teaming arrangements with same partner.  If this is a first time joint effort, each party to the arrangement must provide a list of past and present relevant contracts. Distinguish between work to be performed by each team member, subcontractor, etc..



(c) Subcontractor Consent: Past performance information pertaining to a subcontractor cannot be disclosed to the prime offeror without the subcontractor’s consent. Provide with the proposal a letter from all subcontractors that will perform major or critical aspects of the requirement, consenting to the release of their past performance information to the prime contractor.



(d) Past Performance Questionnaire:  The offeror shall send out and track the completion of the Past Performance Questionnaire for each contract point of contact identified in the Past Performance Surveys.  The responsibility to timely send out and track the completion of the Past Performance Questionnaires rests solely with the offeror.  The offeror shall exert its best efforts to ensure the points of contact submit completed Past Performance Questionnaires directly to the Contract Specialist as specified on the first page of the Past Performance Questionnaire by the closing date of the solicitation.



(e) Documents submitted in response to this solicitation must be fully responsive to and consistent with the following:



1. Requirements of the solicitation, Performance Work Statement (PWS), and government standards and regulations pertaining to the PWS.



2. Evaluation Factors for Award.



3. Part III – Pass/Fail Technical Evaluation-



The offerer will provide a comprehensive description of their proposed technical solution. Submit original and one (1) copy and one (1) electronic copy in .pdf or a Microsoft Word 2003 format.  The number of pages for the technical proposal will be no more than (11) pages.





Total Amount of Pages for the Proposal Package:



The maximum amount of allowable pages (including the price, past performance and technical proposal) cannot exceed (15) pages. Any included technical drawings or diagrams will count as (1) page. Any information included in the proposal past the maximum amount of allowable pages will be not be examined. Complete the necessary fill-ins and certifications in provisions. The provisions  shall be returned along with the proposal, these pages will not be counted against the total page count. 



General Information



INFORMATION REGARDING SUBMISSION OF PROPOSAL:  Hand carried proposals must be delivered to the 95th Contracting Squadron at 5 S. Wolfe Ave., Edwards AFB, CA. 93524 attention Donald Ballaro or Craig Keelen.  The sealed envelope or package used to submit your proposal must show the time and date specified for receipt, the Solicitation Number, and the name and address of the offeror.



Offerors are cautioned that Edwards AFB, CA has visitor control procedures requiring individuals not affiliated with the installation to obtain a visitor pass prior to entrance. SOME DELAY SHOULD BE ANTICIPATED WHEN HANDCARRYING PROPOSALS. Offerors should allow sufficient time to obtain a visitor pass and arrive at the bid depository PRIOR to the time specified for receipt. Late proposals will be processed in accordance with  “Late submission, modifications, revisions, and withdrawals of offers.”

52.212-2 EVALUATION--COMMERCIAL ITEMS



(a)  The government will award a contract resulting from this solicitation to the responsible offeror whose offer conforming to the solicitation will be most advantageous to the Government, price and other factors considered. The following factors shall be used to evaluate offers:



      (1)  Pass/Fail Technical Evaluation

(2)  Past Performance

(3)  Price



Past Performance is significantly more important than price when being evaluated.



 (b)  A written notice of award or acceptance of an offer, mailed or otherwise furnished to the successful offeror within the time for acceptance specified in the offer, shall result in a binding contract without further action by either party. Before the offer's specified expiration time, the government may accept an offer (or part of an offer), whether or not there are negotiations after its receipt, unless a written notice of withdrawal is received before award.



Addendum to :



BASIS FOR CONTRACT AWARD: This is a competitive selection in which competing offerors must pass a technical evaluation of their proposed technology solution, they an past and present performance history will be evaluated on a basis that is significantly more important than price. The evaluation process shall proceed as follows:



A. Initially offers shall be ranked according to price. An offeror’s proposed prices will be determined by multiplying the quantities identified in the Pricing Schedule by the unit price for each item to confirm the extended amount for each.  



B. The lowest priced offer shall be evaluated to insure technical acceptability. The proposals will be evaluated on a Pass/Fail basis.  Technical evaluations will be based on how adequately the proposed technical solution meets the requirements of the statement of work.  If the lowest offer passes the technical, evaluation it will be further evaluated based on past performance. If the lowest offer fails the technical evaluation the next lowest priced offeror will be evaluated and the process will continue (in order by price) until an offeror is judged to pass the technical requirements until all offerors are evaluated.  



C. Using questionnaires, the contracting officer shall seek relevant performance information on all offerors based on (1) the references provided by the offeror and (2) data independently obtained from other government and commercial sources.  Relevant and recent performance includes performance of efforts described in the chart shown below:  











The government reserves the right to seek information on higher priced offerors if none of the lower priced offerors receive a “Substantial Confidence” performance assessment rating.  The purpose of the past performance evaluation is to allow the government to assess the offeror’s ability to perform the effort described in the solicitation, based on the offeror’s demonstrated present and past performance.  The assessment process will result in an overall performance confidence assessment rating of Substantial Confidence, Satisfactory Confidence, Limited Confidence, No Confidence, or Unknown Confidence as defined in , Table 3 shown below:

















Past performance regarding predecessor companies or sub contractors that will perform major or critical aspects of the requirement will be considered as highly as past performance information for the principal offeror. Offerors with no relevant past or present performance history or the offeror’s performance record is so limited that no confidence assessment rating can be reasonably assigned shall receive the rating “Unknown Confidence”, meaning the rating is treated neither favorably nor unfavorably.



C.  In evaluating past performance, the government reserves the right to give greater consideration to information on those contracts deemed most relevant to the effort described in this solicitation.



D.  If the lowest priced evaluated offer is judged to have a “Substantial Confidence” performance rating and is determined to be responsible, that offer represents the best value for the government and the evaluation process stops at this point. Award shall be made to that offeror without further consideration of any other offers.



E.  If the lowest priced offeror is not judged to have a “Substantial Confidence” performance assessment, the next lowest priced offeror will be evaluated and the process will continue (in order by price) until an offeror is judged to have a “Substantial Confidence” performance assessment or until all offerors are evaluated.  The Source Selection Authority shall then make an integrated assessment best value award decision.



F.  Offerors are cautioned to submit sufficient information and in the format specified in the proposal preparation instructions to permit a meaningful assessment of past performance. Offerors may be asked to clarify certain aspects of their proposal or respond to adverse past performance information to which the offeror has not previously had an opportunity to respond.  Adverse past performance is defined as past performance information that supports a less than satisfactory rating on any evaluation element or any unfavorable comments received from sources without a formal rating system.  Communication conducted to resolve minor or clerical errors will not constitute discussions and the contracting officer reserves the right to award a contract without the opportunity for proposal revision.



G.  The government intends to award a contract without discussions with respective offerors. The government, however, reserves the right to conduct discussions if deemed in its best interest.






Assessment Area

Very Relevant

Relevant

Somewhat Relevant

Not Relevant

Scope 

(Reference Sec 1 of Performance based Work Statement (PWS)





Responsible for equipment procurement/ installation exceeding the tasks required by the PWS for equipment procurement and installation.

Responsible for equipment procurement/ installation equal to tasks required by the PWS for equipment procurement and installation.

Responsible for equipment procurement/ installation slightly less than the tasks required by the PWS for equipment procurement and installation.



Magnitude/

Complexity

(PWS Section 3, Estimated Workload Data)

Responsible for equipment procurement/ installation  exceeding the tasks required by the PWS for equipment procurement and installation.

Responsible for equipment procurement/ installation equal to tasks required by the PWS for equipment procurement and installation.

Responsible for equipment procurement/ installation slightly less than the tasks required by the PWS for equipment procurement and installation.



Contract Type

Performance under a Government Firm-Fixed Price Contract.

Performance under a Firm Fixed Price Contract

Any other contract type other than Firm Fixed Price



Contract Environment

Contract performance of equipment installation on a military installation in a secure environment.

Contract performance of equipment installation on a military installation

Contract performance of equipment installation at any U.S. location. 



Current or Recent

(with minimum 

   1 year of 

   Performance)

Current Performance (performance within 12 months prior to issue date of solicitation)

Recent Performance (within 2 years)

Recent Performance (within 3 years)





TABLE 3- PERFORMANCE CONFIDENCE ASSESSMENTS

TABLE 3- PERFORMANCE CONFIDENCE ASSESSMENTS

Rating

Description

SUBSTANTIAL CONFIDENCE

Based on the offeror’s performance record, the government has a high expectation that the offeror will successfully perform the required effort.

SATISFACTORY CONFIDENCE

Based on the offeror’s performance record, the government has an expectation that the offeror will successfully perform the required effort.  



LIMITED CONFIDENCE 

Based on the offeror’s performance record, the government has a low expectation that the offeror will successfully perform the required effort.

NO

 CONFIDENCE

 Based on the offeror’s performance record, the government has no expectation that the offeror will be able to successfully perform the required effort. 





UNKNOWN CONFIDENCE 

No performance record is identifiable or the offeror’s performance record is so sparse that no confidence assessment rating can be reasonably assigned.
                                                    PAST AND PRESENT PERFORMANCE QUESTIONNAIRE


A.  GENERAL INFORMATION: Please correct any information below known to be inaccurate:

Contractor’s Name: ____________________   Telephone Number: ____________________
Address:  __________________________        Fax Number: _________________________
                __________________________        Point of Contact: ______________________
          __________________________
          __________________________

   (The offeror will enter this information and is required to submit questionnaires to the references.)

Project Title and Brief Description of Work: _____________________________________*

________________________________________________________________________*

Contract Number Provided by Offeror: ___________________ Dollar Amount: ________*

Contract Period or Dates of Performance Provided by Offeror: ______________________* Contractor performed as the ( Prime Contractor ( Sub-
Contractor


   * Note:  If offeror holds or has held other contracts with your agency/organization in the last 3 years, please complete separate evaluation
   forms for those contracts as well.


B.  RESPONDENT INFORMATION:

Name of Respondent:    _____________________ Title: ____________________________

Address: _____________________          Telephone Number: ___________________ _____________________________            Fax Number:
_____________________
_____________________________           Email Address: _____________________


C.  E-MAIL COMPLETED SURVEY FORM TO:  brian.wu@edwards.af.mil and ronda.satori@edwards.af.mil


D.  PERFORMANCE INFORMATION: Choose the appropriate letter on the scale (E, G, S, M, U, and N) that most accurately describes the contractor’s
performance or situation. PLEASE PROVIDE A NARRATIVE EXPLANATION FOR ANY RATINGS OF M or U.

|E                     |G                       |S                        |M                   |U                   |N                   |


CONTRACTOR’S NAME: _______________ CONTRACT NUMBER: _______________
   Note:  Include this information on each page of the questionnaire form to ensure there is no mix up in information among contracts surveyed
   for respective primes/subs, etc.
Place an “X” in the appropriate column using the definitions matrix above.

|       |                                                                                  |E   |G     |S   |M   |U   |N   |
|1.     |How well did contractor manage staffing to maintain sufficient quantity of        |    |      |    |    |    |    |
|       |personnel with adequate skill mix to ensure performance throughout contract period|    |      |    |    |    |    |
|       |of performance?                                                                   |    |      |    |    |    |    |
|2.     |How would you rate the contractor’s staff in terms of skill level and motivation? |    |      |    |    |    |    |
|3.     |How effective was the contractor in building and maintaining positive working     |    |      |    |    |    |    |
|       |relationships within the contractor’s own team and with the Government/customer?  |    |      |    |    |    |    |
|4.     |How well did the contractor respond to contract changes in terms of finalizing    |    |      |    |    |    |    |
|       |requirements definition, promptly providing pricing information, and executing the|    |      |    |    |    |    |
|       |change?                                                                           |    |      |    |    |    |    |
|5.     |How effective was contractor’s quality control efforts at ensuring satisfactory   |    |      |    |    |    |    |
|       |contract performance?                                                             |    |      |    |    |    |    |
|       |TECHNICAL CAPABILITIES                                                            |    |      |    |    |    |    |
|6.     |How well did contractor meet contract technical requirements and other contract   |    |      |    |    |    |    |
|       |terms and conditions?                                                             |    |      |    |    |    |    |
|7.     |Was the installation done in a professional, organized, and timely manner?        |    |      |    |    |    |    |
|8.     |Has their product maintained reliability?                                         |    |      |    |    |    |    |
|9.     |Is the system software an effective product that a user could operate effectively |    |      |    |    |    |    |
|       |with minimal training?                                                            |    |      |    |    |    |    |
|       |General                                                                           |    |      |    |    |    |    |
|10.    |Identify the contractor’s overall strengths and weaknesses.                       |    |      |    |    |    |    |
|11.    |What is your overall rating of the contractor’s performance?  Why                 |    |      |    |    |    |    |
|12.    |Is there anyone else we should send this questionnaire to?  Please identify by    |    |      |    |    |    |    |
|       |name, address, organization, and phone number.                                    |    |      |    |    |    |    |
|       |TERMINATION HISTORY                                                               |    |      |    |    |    |    |
|13.    |Has this contract been partially or completely terminated for default or          |    |      |    |    |    |    |
|       |convenience?                                                                      |    |      |    |    |    |    |
|14.    |Are there any pending terminations?                                               |    |      |    |    |    |    |
|       |QUALITY OF PRODUCT OR SERVICE                                                     |    |      |    |    |    |    |
|15.    |Did the contractor’s work measure up to commonly accepted technical or            |    |      |    |    |    |    |
|       |professional standards?                                                           |    |      |    |    |    |    |
|16.    |Assess the contractor’s willingness to improve and correct non-compliance issues  |    |      |    |    |    |    |
|       |and concerns that arose during performance.                                       |    |      |    |    |    |    |
|17.    |Assess the contractor’s ability to perform in accordance with contract schedule.  |    |      |    |    |    |    |
|18.    |Assess the contractor’s ability to provide timely resolution to customer          |    |      |    |    |    |    |
|       |complaints.                                                                       |    |      |    |    |    |    |
|19.    |Was the contractor ever issued Delinquency, Show Cause, or Cure Notices?          |    |      |    |    |    |    |
|20.    |To what extent did the contractor meet the proposed cost estimates?               |    |      |    |    |    |    |
|21     |Would you award another contract to the contractor?                               |YES/NO                           |

CONTRACTOR’S NAME: ___________________  CONTRACT NUMBER __________


Remarks:_________________________________________________________________________________________________________________________________________
_________________________________________________________________________________________________________________________________________________
_________________________________________________________________________________________________________________________________________________
_________________________________________________________________________________________________________________________________________________
_________________________________________________________________________________________________________________________________________________
______________________________

SITE VISIT

FA9301-10-M-A036

CONTRACT FOR BAF DVR UPGRADE

EDWARDS AFB CA



A SITE VISIT WILL BE HELD 17 June 10 at 12:00 p.m.



Site Visit attendees are to meet at 95th Contracting Squadron/PKB, , Bldg. 2800 lobby at 12 p.m.  Transportation will be provided to the site.  



ABSOLUTELY NO PHOTOGRAPHY WILL BE ALLOWED.  



DIRECTION (see attached map)



From the  Fwy. (14):  Exit at   Head east (turn right if coming from the south or turn left if coming from the north).  Continue on   Turn right on   Turn left into the Bldg. 2800 parking lot.



ACCESS REQUIREMENTS



ALL individuals (not to exceed 2 people) wishing to attend the site visit must pre-register.



Individuals that wish to attend the site visit SHALL submit the following information by either fax (661-277-0470) or e-mail (; ) no later than 2:00 p.m. four calendar days before the date of the site visit. In addition, they need to fax a visit request to 661-277-6589/Rita Sokolik.



Name

Date of Birth

Place of Birth

Social Security Number

Citizenship

Green Card Number (if applicable)

Driver’s License Number/State

Vehicle Brand and Model

Vehicle License Number/State



Individuals that are not legal residents of the  will not be allowed access to Edwards AFB. Also, on the faxed document to Rita Sokolik, the visit request must be on company letterhead and signed by President of Company/Human Resources/Security, etc. A person who is listed on the visit request cannot sign the visit request.





If you do not submit the information specified above by the proper date you will not be allowed access to Edwards AFB for the site visit.



Points of Contact:  Brian Wu (661) 277-7448 or Ronda Satori (661) 277-3286. Additional POC: Stephen Brightman (661) 277-8704.







Site Visit

FA9301-10-M-A036

Baf DVR Upgrade

POC:  Brian Wu (661) 277-7443 or Ronda Satori (661) 277-3286







Statement of Work



for the



BAF Test Control Room

Video Recorder Upgrade







24 March 2010







772nd Test Squadron

412th Electronic Warfare Group

Air Force Flight Test Center

Edwards Air Force Base, California 93524







Prepared By

Project Development Division

412 EWG / EWP

















BACKGROUND



Overview

The Benefield Anechoic Facility is a part of the Electronic Warfare Group’s test complex at Edwards Air Force Base.  The BAF was built in the late 1980s, and was originally designed to support testing of the defensive and offensive EW suites on military aircraft.  In addition to numerous laboratories, the BAF houses a large, well isolated chamber that provides a quiet radio frequency (RF) environment that is ideal for testing various receiver- and transmitter-based systems.  Numerous types of tests can be conducted within the BAF chamber, including antenna pattern tests, electromagnetic interference / electromagnetic compatibility (EMI/EMC) tests, and electronic warfare (EW) tests.

To facilitate the orderly conduct of tests and the operation of test equipment, the BAF includes a Test Control Room.  During a test, the TCR hosts both test customers and BAF operations personnel.  It is configured with displays, control equipment, and test equipment as required to conduct the test.  A substantial amount of test data and information is presented in the form of displays, both from computer systems and from aircraft systems, and test equipment.  Video signals from these displays are routed to the TCR for display to test personnel; there they are also routed to a video recording system.

The objective of this project is to provide a new video recording system that provides higher performance, and addresses a number of technical and operational shortfalls with the existing video recorders in the TCR.  The upgraded system will allow the EW Group to provide higher video quality products to test customers, as well as provide for more effective handling and processing of video data during and after tests.

Current System and Capabilities

System Layout

The current recording system is configured to record on eight (8) standard home DVD recording units.  Each of the eight (8) units has a dedicated backup DVD recorder daisy-chained to the primary DVD recorder, making a total of sixteen (16) recorders in the system.  In order to synchronize successfully, the DVD recorders are controlled via an infrared (IR) interface physically connected to the IR port on each unit.  

Pre-Test Preparation

During a test, video may come from a variety of sources including spectrum analyzers, the System Under Test (SUT), computer monitors, and video capture devices.  Prior to the test, the customer must prioritize which video sources will be recorded.  Although the TCR is monitoring and collecting video from an array of input sources, it is limited to recording video from eight (8) sources.

Operationally the user configures the DVD recorder to record from one (1) of two (2) inputs based on which sources have been selected.  For example, a spectrum analyzer is recorded from the S-Video input because it is a higher resolution signal.  On the other hand camera outputs, which are NTSC video signals (and lower in resolution), are recorded using either RCA or S-Video connections.  Computer displays, that are much higher resolution than either of the above, are converted to S-Video for recording.

Prior to switching on the recording equipment, the operator configures the IRIG time to overlay on top of the video.  This is used during post test analysis to refer back to test points of interest.  

Recording Time

In normal BAF test operations, the DVD recorders are configured to record for four (4) hours on each disk.  Tests are normally run for eight (8) hours. With eight (8) sources needing two (2) disks each, sixteen (16) disks must be prepared with headers prior to use in the DVD recording units (the headers identify the classification of the test for example).  Eight (8) are placed in the primary recorder and eight (8) are placed in the backup recorder.  A typical eight (8) hour workday produces thirty-two (32) DVDs.

Video Resolution

The video recorders are currently configured to record at standard DVD resolution, or roughly 525 lines.  With the four (4) hour recording time setting, there is also a substantial amount of compression applied to the video that results in some loss of video quality.  Also, In addition, using DVD recorders prevents the user from capturing sources at their native resolution.  The result is higher resolution sources are recorded with a loss of quality, and lower resolution sources are recorded at a higher quality than required.  Both introduce artifacts into the video.  

Test Conduct

Once inputs are selected along with the setup of IRIG time and disk headers, the test video recording operator initiates recording using an IR interface that directly connects to all sixteen (16) recorders.  If a recorder does not engage, the operator is alerted of the false start, and repeats the record process until all sixteen (16) players begin recording.  The only indication of a false start comes from the front panel display of each DVD recorder.  

Once recording has started, the operator can verify that each recorder is capturing video successfully by viewing the live video output of each recorder on a rack-mounted monitor.  The monitor also allows the operator to manually toggle between the eight (8) video inputs. 

The video recorders operate continuously during a test.  If the test is stopped due to a failure or incorrect results, the recorders continue to record while test operators troubleshoot the problem.  In addition, the recorders are never switched to another input source midway through a test.  The same configuration is used for an entire four (4) hour test block.

Post Test

The test normally stops at the four (4) hour point, because the video disks have been filled to capacity and to give operators a break.  During this time, the disks are finalized, or closed out. No additional video can be recorded to them.  Labels are applied to each disk and individually placed into a sleeve.  During the finalization process, if a disk does not closeout correctly, or becomes corrupted, the operator re-burns a copy using the backup recordings and an external DVD burner connected to a control computer.  The operator then places new initialized disks into the player to prepare for the next four (4) hours of testing.

This process is repeated until the test is completed.  On average, 160 disks are produced every week.  Test duration ranges from one (1) week up to months.  At the completion of testing, one set of disks is given to the customer.  The other is kept in the EWG configuration management (CM) office for archiving.

Post Test Analysis

The customer can use a standard DVD player or computer video viewer (like Windows Media Player) to review and analyze the material on the disks.  In many cases, an analyst is interested in only small portions of video, for instance only the periods of time directly before, during and after points of interest.  The analyst can use the IRIG time overlay to identify their desired point of interest.  Once found, they can use the video review program to manually scroll through the video surrounding the point of interest, or manually move from frame to frame.  

Shortfalls of Current Recording System

Listed below are shortfalls of the current system.  These shortfalls drive the requirements listed in Section 3 of this document.  

System Layout

The eight (8) DVD players are not 100% reliable.  A corrupt disk is not uncommon.  While it is easy to substitute the backup as the primary and record another copy as a new backup, this is not a desirable trait in the system.

Pre-Test Preparation

Test setup is a challenge because of the inability to dynamically change the configuration.  As mentioned earlier, once configured the setup cannot be changed until the end of a four (4) hour block.  Prior to beginning a recording, the operator must manually select the input for each of the eight (8) devices. 

A standard test workweek produces 160 disks.  Each disk must be manually labeled and manually placed into individual storage sleeves.  This is labor intensive, presents the risk of mislabeling, and contributes to non-test downtime between recording sessions. 

Recording Time

There is a direct correlation between the recording time available and resolution used on the disks.  To achieve even marginally acceptable recording quality for medium resolution video, the recorders would need to be set for a one (1) or two (2) hour recording time.  However, to prevent test downtime due to replacing DVD media, the recorders must be set to record for four (4) straight hours.  Hence, the operational reality requiring at least four (4) continuous hours recording time prevents the system from being capable of recording at the required resolution.

Video Resolution

As stated above, the requirement to provide at least four (4) continuous hours of recording time mandates setting the recorders to a reduced resolution capability.  This specifically impacts the ability to record test equipment displays (e.g., from spectrum analyzers and oscilloscopes that output at higher resolutions) at a readable level of quality.  Higher resolution displays, such as computer displays, must additionally be “down-converted” to be recorded as S-Video signals, resulting in an even further reduction in quality.  This is the single most critical shortfall of the current recording system.

Test Conduct

The test operator must start each recorder and ensure that all have started.  If one does not start, the operator must restart until all units are operating.  

During a test, the operator can only verify that each recording unit is working properly by manually checking a single video on a single monitor screen or via the display located on each face of the DVD recorder unit.  If a unit fails, the operators will be unaware until the next time that device is checked using the single screen.  The elapsed time between failure and discovery may be extensive.  

There is wasted recording space during the tests.  The recorders record continually, even during a failure or troubleshooting.  (This is done to avoid having to close out the recording chapters on each recorder, and resynchronize all sixteen (16) recorders to a new chapter when resuming)  Hence, if the test is stopped twenty (20) minutes into a test for troubleshooting, and testing does not resume until two (2) hours later, there are two (2) hours of “empty” video on the recording medium (the DVDs).

Post Test 

The DVDs take time to be finalized, and if a disk is corrupted it must be burned again.  This means that video data is not immediately available for any purpose, such as for internal review or processing, or for midday review with a customer.

Numerous individual disks are produced during tests.  These must be handled properly, protected according to classification level, and stored in their final location.  Processing 160 DVDs per week is tedious, and handling them is cumbersome.  

Post Test Analysis

The use of standard commercial DVDs as a recording medium has the advantage of allowing video data to be viewed on normally available devices like a computer or a standalone DVD player.  However, since the DVD is a permanently recorded medium (i.e., it cannot be revised after recording) the ability to perform editing functions on recorded video is limited.  Functions such as extracting “clips” of video and compiling multiple clips into a composite clip must be performed externally to the DVD, with the final product placed on a new medium.  No method currently exists to produce a video data product for a customer other than “everything we recorded.”

Access speeds for DVD media are also slower than many other media (such as hard disk storage), so finding, accessing, manipulating, and copying video using DVDs is slower than optimum.

2 	Safety and Health Standards.  



The contractor shall initiate and maintain programs to comply with the    provisions of	 OSHA concerning entry requirements in confined spaces and handling potential hazardous substances, Appendix 3, List of Federal Regulations Pertaining to Liquid Waste.  



2.1 Safety and Health Discrepancy Tracking



The Contractor must inspect their facilities for safety and health compliance within 30 days after award and every month after that. The Contractor must provide a list of safety discrepancies with references and corrective action to take to the CO.  The CO will forward the report to AFRL Det 7/SE to validate if it is an USAF Safety compliance discrepancy.  The Safety office will forward its review to the CO in a timely matter.  The CO and COR/QAP will attempt to correct the validated safety discrepancies in a timely matter.  It will be the responsible of the Contractor eHASP manager to brief their workers of the hazards (if any) and the operational mitigation plan to avoid the hazard.  It will be the joint responsibility of the eHASP Manager, CO and COR/QAP to track and close items as necessary.



2.2 	Voluntary Protection Program 



Definitions:



Applicable Contractors:  VPP requirements only apply to contractors whose employees work more than 1000 hours per quarter on a government installation.



Days Away, Restricted, and or Transfer Case Incident Rate (DART):  Number of recordable injuries and illness cases per 100 full-time employees resulting in days away from work, restricted work activities, and/or job transfer that a site has experienced in a given time frame.   Total Case Incidence Rate (TCIR).  Total number of recordable injuries and illness cases per 100 full-time employees that a site has experienced in a given time frame.



2.3	Implementation



(a)  Edwards AFB is in the process of pursuing VPP recognition or has already been recognized under the OSHA Voluntary Protection Program (VPP).  VPP impacts all “applicable contractors” operating on Air Force Installations.  It is the Contractor’s responsibility to ensure its employees and managers have a comprehensive understanding of VPP as well as full compliance with OSHA requirements.  Contractors (to include applicable contractors), whether regularly involved in routine site operations or engaged in temporary projects such as construction or repair, must follow the safety and health rules of the installation or VPP site.  Detailed information is available on the OSHA website at .



(b)  Applicable Contractors are required to submit their TCIR and DART rates and OSHA Form 300A annually to the contracting office for consolidation and submission as part of the installation’s annual VPP Safety and Health Management report.  TCIR and DART rates are due by the 15th of January of each year.



(c)   An applicable contractor’s Quality Control Plan (QCP) must identify the processes and procedures the Contractor will use to track compliance with the Safety and Health Plan, and the process and procedures that will be used to correct violations.



(d)  It is the applicable contractor’s sole responsibility for compliance with the Occupational Safety and Health Act (OSHA) (Public Law 91-596).  The Contractor must submit a Safety and Health Plan and corresponding site safety checklist to the contracting officer 10 days after contract award.  The Contractor’s plan shall include appropriate measures to ensure the Contractor reacts promptly to investigate, correct and track alleged safety & health violations and/or uncontrolled hazards in contractor work areas.  Additional, installation specific references and policies may be included /attached to this section.  The plan shall:



1.      Demonstrate a management commitment to employee safety and health;



2.      Identify the application of the safety and health plan to subcontractors;



3.      Identify the roles and responsibilities of the following individuals:



i.      Management;

ii.     Supervisors;

iii.    Employees;

iv.     Safety Coordinator;



4.      Identify applicable safety rules and regulations;



5.      Include a worksite hazard analysis to include base-line hazard identification and required control measures;



6.      Include a job site analysis to include hazards of tasks required to control measures;



7.      Identify employee safety and health training requirements and the documentation process;



8.      Include a workplace inspection frequency, to include indentifying the individual conducting the inspections;



9.      Include employee hazard reporting procedures;



10.  Identify individual(s) responsible for corrective action hazards;



11.  Identify first aid/injury procedures;



12.  Identify procedures for accident investigation and reporting;



13.  Identify emergency response procedures; and



14.  Identify the process for tracking controlled hazards in contractor work areas



(e)  An applicable contractor is responsible for establishing these requirements for all subcontractors who qualify as subcontractors under the resulting contract.



2.4 Upon employment termination or contract end, the contractor shall retrieve all government issued identification badges and passes and turn them in to Security Forces.

2.5 The contractor shall maintain a current listing of employees.  The list shall include the    following information as a minimum:  employee names, social security numbers, and security clearance levels.  The government reserves the right to request additional information if necessary.  The contractor’s security manager shall validate this list and provide it to the CO, QAP, and servicing security forces organization before contract start date.  An updated listing shall be provided upon change of employees.  



2.5 Contractor Personnel.  The contractor shall have a manager specifically assigned to support this contract available within two hours to meet with government personnel during required service hours.  All contract employees shall be able to speak and read the English language. current.    



2.6 General Information.



2.6.1 Quality Control.  The contractor shall develop and maintain a quality control plan (QCP) to ensure the units are maintained in sanitary condition IAW commercial standards.  The contractor shall develop and implement procedures to identify and prevent defective services from reoccurring on all services required by this SOW.  As a minimum, the contractor shall develop quality control procedures that address the areas identified in the Services   Summary.  The contractor’s Quality Control Plan shall be written and submitted within 30 calendar days after award of the contract.  The Contractor shall identify their quality control representative to the government PM and QAP along with a 24 hour phone number.



2.6.2 Quality Assurance.  The government will evaluate the contractor’s performance by appointing QAP to monitor contractor performance to ensure services are received.  The QAP will evaluate the contractor’s performance through intermittent on-site inspections of the contractor's quality control program and receipt of complaints from base personnel.  The government may inspect each task as completed or increase the number of inspections if deemed appropriate because of repeated failures discovered during inspections or because of repeated customer complaints.  Likewise, the government may decrease the number of inspections if performance dictates.  The government will also receive and investigate complaints from various customers located throughout the installation.  All complaints shall be submitted to the PM and QAP.  



2.6.3 Hours of Operation.  The Government’s normal working hours are from 7:00 a.m. to 4:00 p.m. Monday through Friday except for Federal Holidays.



2.6.4 Work Schedules.  The contractor shall schedule and arrange work to ensure the least interference with normal government business operations.  In those cases where some interference is unavoidable, the contractor shall make every effort to minimize the impact of the interference and its effects.



2.6.5 Security Requirements.  The contract manager shall ensure each employee obtains all applicable pass and identification items for contractor personnel and non-government owned vehicles.  These forms are issued by the Security Forces’ Pass and Identification office located in Building 3000, Yeager Boulevard.  Failure to obtain necessary items could result in denial of employee entry to the base.



2.6.5.1. Contractor personnel shall use the AF Form 1199, USAF Restricted Area Card, for unescorted entry into USAF Restricted/Controlled areas on Edwards AFB according to AFI 31-101, as supplemented.  The contractor must, within five (5) days after contract award or the hiring of new employees, comply with the requirements in paragraph (d) of the Contractor Access to Air Force Installations clause, AFFARS 5352.242-9000.  All contractor employees shall abide by all installation security regulations.



3.	Integration Requirements



3.1 Wiring & Connectivity



Provide wiring and connectivity per the attached Connectivity Chart and Video Recording System (VRS) Block Diagram.  Provide all cables, connectors, and other materials needed to implement the required VRS configuration. NOTE: Below is a system level drawing containing the main system components.  It is not a model of exactly what the system requires.  We are open to adjustments that will enhance system quality.  The existing Cheetah Router must be expanded to accommodate this effort.  This part must remain as part of the system.



All wires and cables shall be labeled 2” from end with to/from shown, left to right facing the terminations with the numbering and locations provided on the wire list.



Certain cables will be required to have specific jacket coloring. It is required that the cables match existing color codes in the current TCR equipment room. 



All cables should be cut to fit so that excess cable is minimized but shall allow for chassis removal for servicing.



All classified wire video cables shall be labeled with 1” red tape every 36” along the cable. They shall be separated from other unclassified cables by at least 2”.





NOTES:   	-EXISTING CHEETAH WILL BE EXPANDED

		-ALL OTHER EQUIPMENT SHOWN IS CFE

-USB (OR FIREWIRE) PORT ON VIDEO EDITOR AND DVR FOR CUSTOMER EXPORT TO USB (OR FIREWIRE) HARD DRIVE

3.2 Video System

Procure, install and connect all video equipment, control interfaces, and video system components including connectors and wire/fiber.  Installation will include contractor furnished equipment (CFE) as shown on the VRS equipment requirements list.   

Interconnect the video recorder system to the Cheetah Digital Video Routing Switcher.  

 “Small” electronic components, such as video fiber optic converters and their power supplies, shall be mounted cleanly on rack-mounted trays.

Perform functional checkout.

3.3 Monitors

Procure, install and connect the DVR monitors directly to the DVR monitor outputs.  These shall be Marshall, or equivalent, monitors mounted four (4) across on a standard 19” rack panel.

Perform functional checkout of the system wiring.

3.4 Audio System

Audio input is provided from currently installed Clear-Com intercom system.

Connect the Clear-Com audio intercom audio channels to the VRS system channel inputs.

Provide and install wire/fiber, connectors for both CFE and GFE components (no new audio equipment is to be purchased).

Utilize accepted color coded cable and terminations for all network connections.

The removable drives shall be accessible to the test operators (appropriate height from ground level and drive door clearance).

Perform functional checkout.

3.5 Time Code Inputs



Interface to existing, installed time code.

Install the wiring to the GFE time distribution equipment.

Perform functional checkout.

3.6 Equipment Rack



Install the new equipment into the existing rack, which has a depth of 24”, if possible.  

If the product to be used cannot fit onto the existing rack, procure one equipment rack large enough to accommodate the upgraded system to replace existing rack #R-7.

The new rack should match the other existing racks, should be compatible with 19” wide equipment and have room for the wiring on the sides.

The current racks are Middle-Atlantic WRK series, 44 RU in height and have rear doors with filters, fans in the deep racks, and AC power strips.   

The new rack shall maintain the same standards to assist optimized internal rack cooling based upon the equipment designated for each rack.  The new rack shall utilize the existing 30 amp 120 VAC twist connector as the master plug that connects into the under floor power receptacle.

The new rack shall be grounded to the facility ground utilizing the existing rack grounding equipment.  Ensure rack layouts for the equipment are optimum for use by user technicians.

The new rack shall support adequate cooling and ventilation for all equipment running at full load and continuous operation.

Provide matching blank panels to cover any unused rack space 

	





 

         	       Current Rack R7 Layout

3.7 PESA/Cheetah Systems

Procure, install, and checkout expansion to the existing PESA Cheetah CH128X/6464 HDMR/FIB Router with 16 channel output and matrix card.



3.8 Drawings

Prepare and provide overall system wiring diagrams and wire lists to best commercial practice.  

Connection and wiring diagrams will be updated to reflect new installation or changes.

Possible short notice site access restrictions may occur during certain times.



VRS CONNECTIVITY CHART







3.9 Program Reviews



The integrator shall conduct the following design reviews:

 Kick-off meeting

Preliminary Design Review

Critical (Final) Design Review

Final Government approval required prior to purchase of equipment & installation









3.10 Acceptance Test



The contractor shall perform an acceptance test to demonstrate the capabilities of the system to meet the Government’s defined requirements.  



3.11 Warranties



The equipment provided by the contractor shall maintain a factory warranty of at least one year after system acceptance.



The installation shall be warranted to ensure reliability through operational use of no less than one year after system acceptance.



3.12 System Level Requirements



See below for the system level technical requirements.


NOTE:  How the contractor meets the requirements is at their own discretion.  Adjustments are encouraged that will enhance the system without greatly increasing the cost.



SYSTEM LEVEL REQUIRMENTS







SYSTEM

CFE

CHANNELS/UNITS

CONNECTION

REMARKS

1

DVR

Yes

8 or 12 Channels

Cheetah, IRIG-B, Monitors, Audio, Remote Control, Ethernet, Backup Recording

With remote controls.

2

Cheetah Expansion

Yes

16 Channels minimum

To/From:  Cheetah CH128X/6464 HDMR/FIB Router

Current is 64 channels

3

PESA V-5 TX Fiber

Yes

8 or 12 Units

To:  PESA Cheetah CH128X

Compatible with existing V-5

4

PESA V-5 RX Fiber

Yes

8 or 12 Units

From:  PESA Cheetah CH128X

Compatible with existing V-5

5

Clear-Com Intercom

No

8 or 12 Channels audio

To/From:  DVR(s)

Existing Intercom

6

DVR Monitors, one per channel

Yes

8 or 12 Monitors

4 to a panel, Marshall Company or equivalent

19” Rack Mount, 1920x1200 Max 

7

Video Editor 

Yes

1 or 2 Units for 8 or 12 Channels.

To/From:  DVR(s)

Placed in a different location

8

IRIG-B Time Code

No

1 or 2 Channels

To/From:  DVR(s)

Existing

9

Backup Recorder and Recording Media

Yes

8 or 12 Channels

To/From:  DVR(s)

USB/NAS/Firewire





Video Recording



The Video Recording System (VRS) shall record from no less than eight (8) independent sources.  The goal is twelve (12).



Each channel of the VRS shall be able to accept video from NTSC composite, S-Video, and RGB (analog/VGA or DVI) sources.



The VRS shall be fitted with, at a minimum, the following inputs per channel:

S-Video on standard S-Video connector

Composite on BNC connector

RGB video on DVI-I, DVI-D, or DB-15



The VRS shall provide the capability to select, for each input channel, between composite, S-Video, and RGB sources.



The VRS shall support, at a minimum, the following RGB video input resolutions consisting of the following commonly used 4:3 and 16:9 ratios:

	640x480 pixels		1440x900
	800x600			1680x1050

	1024x768			1920x1200

	1280x1024			

	1600x1200	



The VRS shall support RBG video input signals with a refresh rate of up to 75Hz.



The VRS shall auto-sync to each input video signal.



The VRS shall allow the user to select the recording resolution of each input channel.			



The maximum resolution of a VRS recording shall be at least 1280 x 1024 pixels for each channel.



The minimum resolution of a VRS recording shall be 640 x 480 pixels or less for each channel.



The VRS shall be able to record using up to a 24 bit color depth for RGB sources.



The VRS shall support adjustable recording rates for each channel from one (1) frame per second (fps) to the maximum frame rate of the incoming video.



The VRS shall allow the user to select the recording frame rate of each source independently.



The VRS shall give the user the ability to turn off or on video compression for each recording source independently from other recording sources.



The compression scheme shall be adjustable to allow the operator to control the quality of the recorded video.



The VRS shall provide for recording all channels simultaneously, with no visible loss of picture quality and no dropped frames in the recording, with all channels recording, with input signals comprised of RGB video, 1280 x 1024 resolution, at 30 fps at a minimum.



The VRS shall minimize the amount of video lost in the event a problem occurs during recording.



The VRS shall allow the user to select the filename and destination of the video recording.



The VRS shall allow the user to view video in real time on a separate monitor to see what is being recorded.



The VRS shall allow the user to display the name of the video as overlay on a recording for each channel.



The VRS shall allow the user to select the font style, font color, and background color for the display name.



The VRS shall allow for the user to specify placement, size, and position of the video name on the video.



The VRS shall allow the operator to place a user defined standard full screen “header” and “footer” message (e.g., noting classification level and usage restrictions) approximately one (1) minute long at the beginning and end of a video recording.  



The VRS shall provide the user the ability to overlay recorder configuration information on recorded video (e.g. frame rate, resolution, and source).



The VRS shall immediately notify the user if a video signal is lost during live recording.



The VRS shall notify the user if a video file already exists of the same name.



If the user attempts to create a video recording that overwrites an existing recording, the VRS shall prompt the user to confirm the action, and shall create a backup copy of the recording being overwritten (i.e., the original recording shall not be lost).



Audio Recording 



The VRS shall provide one channel of recorded audio for each video channel.



Each VRS audio channel shall support connection to a 0 dBu, 100 Ohm balanced source, and a 1 volt RMS, low impedance unbalanced source.



The quality of the recorded audio shall remain constant regardless of the recorded video frame rate.



The VRS shall provide an audio level indicator to the user that audio is being recorded.



Time Stamp



The VRS shall include a video time stamp to allow for later search, selection, and identification of the recorded video.



The VRS shall support a time stamp based on an IRIG-B (AM input) time code.



The VRS shall provide a unique accurate time stamp for each frame of the recorded video for each video channel.



The VRS shall allow the user to overlay the time stamp for each video source.



The time stamp shall be in the form of ddd-hh:mm:ss.ss using IRIG based time and Julian calendar day.



The VRS shall allow the user to specify placement, size and position of the time stamp on the video.



The VRS shall allow the user to select the font style, font color, and background color for the time stamp name.



The VRS shall have the option to either turn off or on the overlay time stamp.



Recording Capacity



The VRS shall provide storage to allow continuous recording for ten (10) hours per day, given the following input composition:

Two S-Video inputs, 60Hz interlaced, at 30 fps +

Four RGB inputs, 640x480 resolution, 60Hz non-interlaced, at 30 fps +

Two RGB inputs, 1280x1024 resolution, 60Hz non-interlaced, at 30 fps

and shall provide for at least five (5) days total video storage.



The VRS shall provide a visual indicator to the test operator of how much recording time remains on the recorder.



The VRS shall provide a visual indicator to the user of how much storage remains on the recorder



The VRS shall display to the user how much time has elapsed since the start of the recording



Recording Medium



The recording medium for the VRS shall be removable for all classification levels.



The VRS shall allow the operator to either record locally to a hard-disk or to a Network Attached Storage (NAS) in real-time.



Playback



The VRS shall provide separate playback video outputs for each channel for video distribution systems and monitors.



The VRS shall provide video outputs for runtime monitoring purposes.



The VRS shall provide separate audio playback for each channel.



The VRS shall provide audio playback synchronized with the corresponding recorded video.



The VRS audio output connectors shall be the same as the audio input connectors.



The VRS shall provide the capability to playback any or all channels simultaneously and time synchronized.



The VRS shall be able to playback recorded channels in real time with no dropped frames.



The VRS shall be able to play back multiple channels simultaneously with no lag or dropped frames.



The VRS shall provide the user the capability to scroll to any point in time in the video during playback.



The VRS shall allow playback immediately after recording has stopped.



Review



The VRS shall provide a method, both through the recorder unit itself and a dedicated review workstation, the capability for the playback of recorded video, selection of video clips, patching together video clips, and export of recordings of clips to external mediums or devices.



The VRS shall provide the user the capability to scroll to any point in time in the video during review.



The VRS shall allow for review within one (1) hour after a four (4) hour recording session on the review station.



The VRS shall allow the user to extract a portion of video from the original recording.



The VRS shall allow the user to save the extracted video in its original format or a trans-coded format (including MPEG video).



The VRS shall allow the user to add a security header and footer to the extracted clip.



Recorder Control



The VRS shall allow for an operator to take control of the recorders and the functions associated with them.



The VRS shall incorporate the following controls for each recorder:

Start recording,

Stop recording,

Start playback,

Stop playback,

Pause recording, 

Resume recording,

Pause playback,

Resume playback,

Fast forward playback, 

Fast reverse playback.



The VRS shall allow the operator to control any individual recorder or all recorders simultaneously.



The VRS shall allow an operator to remotely control recorders from one terminal or graphical user interface (GUI).



The VRS control capability shall be accessible through any one or more of the following methods: a Windows PC based machine, remote via web browser, remote via windows remote desktop, or remote via X-Windows interface.



The VRS shall provide an option to display recorder channel configuration information (e.g., frame-rate, resolution, input sources, file information).



Backup



The VRS shall provide for backup recording via one of the following methods:

Piggyback recorder,

Redundant capture and record,

Redundant disk or storage medium, or Redundant NAS.



 Archive



The VRS shall provide the capability to copy recorded video to an archive medium for long-term storage.



The VRS shall provide the capability to allow archived video to be placed back on the recorder for restoration or to create copies.



The Archive medium shall be either optical, disk, or tape.



Export



The system shall provide the capability to export recorded video to another medium for distribution to the customer.



The exported video shall be playable with a commonly available video coding standard.



The exported (encoded) video shall be viewable on a non-proprietary Windows based system viewer.



Review Station



The system shall include a dedicated review work station, separate from the recording unit(s).



The review station shall be able to playback and utilize the original recordings.



The review station shall be able to export a copy of video in no more than one (1) hour for every one (1) hour of video for each channel.



Other Technical Requirements



The VRS shall be delivered with all media and software required to build new system and data disk drives, utilizing user purchased commercially available hardware, without reliance on the system developer or vendor.



The VRS shall provide a network interface for connection to 100/1000 Ethernet CAT6 distribution



The recorders shall be rack mountable and must fit into a standard 19 inch rack.



The VRS shall be designed to fit into existing TCR rack space, approximately 4 rack sections of 20U each.



The VRS shall come with three (3) hard copy sets of user manuals and three (3) sets of electronic user manuals covering system operation, setup, and configuration.



The contractor shall provide onsite training for users.